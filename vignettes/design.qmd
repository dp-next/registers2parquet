---
title: Design
vignette: >
  %\VignetteIndexEntry{Design}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

## Conversion process

::::: columns
::: column
```
bef2018.sas7bdat
bef2019.sas7bdat
bef2020.sas7bdat
bef2021.sas7bdat
bef2022.sas7bdat
December_2023/bef2022.sas7bdat
December_2023/bef2023.sas7bdat
```
:::

::: column
```
bef/
├── year=2018/
│   └── part-0.parquet
├── year=2019/
│   └── part-0.parquet
├── year=2020/
│   └── part-0.parquet
├── year=2021/
│   └── part-0.parquet
├── year=2022/
│   └── part-0.parquet
└── year=2023/
    └── part-0.parquet
```
:::
:::::

## Parallel processing

Each register-year pairing (including duplicate years) are sent to a
separate process. That way, rather than converting 1000+ registers one
after the other, it can be split into chunks based on the number of
available CPU cores. For instance, if you set the number of "workers"
(cores) to 4, than the 1000+ registers will be split into 4 groups and
each group will be processed simultaneously on its own core.

There are some overheads to parallel processing, so it works best with a
large number of files and when the files are also relatively large, or
if the processing time per file is substantial.

## Visual representation of data flow

```{mermaid}
%%| fig-alt: "A flowchart showing the data flow within `registers2parquet`, showing how SAS files are imported, potentially joined, and then exported as Parquet files with a year partitioning."
%%| file: flow.mmd
```

## Functions

`registers2parquet` includes two main functions and multiple helper
functions. Each function is listed below with a short description.

### Main functions

#### `convert_to_parquet()`

This is the main function of the package. It takes as input one or more
SAS files, reads them, merges them if necessary, and saves them as a
Parquet file. If multiple files are given, the function looks for a year
in the file names to use as partitioning. The function also removes any
duplicate rows before saving the data. The output is the path to the
created Parquet file(s) as specified by the `output_path` parameter.

<!-- TODO: Add link to reference documentation when ready -->

``` r
#' Convert register SAS file(s) and save to Parquet format
#'
#' If multiple paths are given, the function looks for a year (4 digits) in the
#' file names to use the year as partition, see `vignettes("design")` for more
#' information about the partitioning.
#'
#' If multiple files are given with the same year, they are merged with a full join.
#'
#' If any duplicate rows found, they are removed before saving to Parquet.
#'
#' @param input_path A character vector with the absolute path to the register SAS file(s).
#' @param output_path The path with the directory to save the output Parquet file to.
#'
#' @returns Returns a character vector with the path to the created Parquet file(s)
#'   (from `output_path`), so it can easily be used in a targets pipeline.
#' @export
#' @examples
#' \dontrun{
#' convert_register_to_parquet(list_sas_registers("202020")), "output_directory/")
#' }
convert_register_to_parquet <- function(input_path, output_path) {
  # Initial setup and checks.
  fs::file_exists(input_path)
  checkmate::assert_character(input_path)
  checkmate::assert_character(output_path)
  checkmate::assert_scalar(output_path)
  fs::dir_create(fs::path_dir(output_path))

  # Read SAS file(s) and full join.
  data <- read_sas_files(input_path) |>
    # Add year column if possible.
    add_year_col() |>
    # Deduplicate identical columns.
    dplyr::distinct() |>
    # TODO: How to handle history/almost-duplicate rows?
    # Write to Parquet.
    # TODO: Might have to do some processing of the dates. If so, create a helper function for this.
    # mutate(across(where(~inherits(.x, what = "date")), as.character))
    # mutate(across(where(lubridate::is.Date), as.character))
    arrow::write_parquet(output_path)
    # TODO: How is the year partitioning handled in/passed to arrow::write_parquet()?


  if (length(input_path) > 1) {
    cli::cli_alert_success(
      "Finished merging and saving the duplicate {.val {fs::path_file(input_path)[1]}} files as a Parquet file."
    )
  } else {
    cli::cli_alert_success(
      "Finished saving {.val {fs::path_file(input_path)}} as a Parquet file."
    )
  }

  output_path
}

read_sas_files <- function(input_path) {
  input_path |>
    purrr::map(haven::read_sas) |>
    purrr::reduce(dplyr::full_join)
}

add_year_col <- function(data) {
  year <- get_database_year(input_path) |>
    unique()
  if (!is.na(year) & year %in% 1969:2030) {
    data <- data |>
      dplyr::mutate(year = year)
  } else {
    cli::cli_alert_info(
      "Could not determine year from file name(s). Will not partition by year."
    )
  }
  data
}
```

#### `read_register(register_name, project_id)`

Reads a Parquet register from the specified path and returns it as a
DuckDB table.

<!-- TODO: Add link to reference documentation when ready -->

### Helper functions

#### `list_sas_registers(project_id)`

Lists all SAS files in a given directory. It returns a character vector
of file paths with the `.sas7bdat` extension at the given directory.

<!-- TODO: Add link to reference documentation when ready -->

#### `list_parquet_registers(project_id)`

Lists all Parquet files in a given directory. It returns a character
vector of file paths with the `.parquet` or `.parq` extension at the
given directory.

<!-- TODO: Add link to reference documentation when ready -->
